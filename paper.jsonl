{"id":"tancik2020fourier","title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains","authors":"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.10739","keyPoints":"Fourier feature mapping enables MLPs to learn high-frequency functions; Overcomes spectral bias using Neural Tangent Kernel theory; Simple mapping: γ(v) = [cos(2πBv), sin(2πBv)]^T","keyHypotheses":"Standard MLPs fail at high frequencies due to spectral bias; Fourier features can transform NTK into stationary kernel with tunable bandwidth","strengths":"Theoretical foundation via NTK; Simple and effective method; Wide applicability to computer vision tasks","weaknesses":"Requires careful frequency selection; May not capture all signal patterns","citation":"Tancik, M., et al. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS.","notes":"Seminal work for positional encoding in INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"sitzmann2020siren","title":"Implicit Neural Representations with Periodic Activation Functions","authors":"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.09661","keyPoints":"Uses sine activation functions for representing complex signals; All derivatives of sine are sine functions; Special initialization scheme for stable training","keyHypotheses":"Periodic activations better represent natural signals than ReLU; Derivative access enables solving differential equations","strengths":"Smooth representations; Derivative access at any order; Works across multiple domains","weaknesses":"Sensitive to initialization; Limited theoretical analysis","citation":"Sitzmann, V., et al. (2020). Implicit neural representations with periodic activation functions. NeurIPS.","notes":"Alternative to Fourier features for INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"mildenhall2020nerf","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","authors":"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng","journal":"ECCV","year":"2020","doi":"","url":"https://arxiv.org/abs/2003.08934","keyPoints":"Represents 3D scenes as continuous 5D radiance fields; Uses positional encoding and volume rendering; Hierarchical sampling with coarse and fine networks","keyHypotheses":"Scenes can be represented as continuous functions mapping coordinates to color and density; MLPs with positional encoding can capture complex 3D structure","strengths":"Revolutionary 3D representation; High-quality novel view synthesis; Continuous representation","weaknesses":"Slow training and inference; Requires many input views","citation":"Mildenhall, B., et al. (2020). NeRF: Representing scenes as neural radiance fields for view synthesis. ECCV.","notes":"Foundational work showing power of INRs for 3D reconstruction","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"chen2022tensorf","title":"TensoRF: Tensorial Radiance Fields","authors":"Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su","journal":"ECCV","year":"2022","doi":"","url":"https://arxiv.org/abs/2203.09517","keyPoints":"Models radiance field as 4D tensor with factorization; CP decomposition and Vector-Matrix decomposition; Fast reconstruction with compact models","keyHypotheses":"Tensor factorization can accelerate INR training while maintaining quality; Low-rank structure exists in radiance fields","strengths":"Significant speedup over NeRF; Compact model size; Multiple factorization options","weaknesses":"Still requires careful hyperparameter tuning; Limited to radiance field domain","citation":"Chen, A., et al. (2022). TensoRF: Tensorial radiance fields. ECCV.","notes":"Key paper for tensor factorization in neural fields - directly relevant to matrix factorization","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"fridovich2023kplanes","title":"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance","authors":"Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, Angjoo Kanazawa","journal":"CVPR","year":"2023","doi":"","url":"https://arxiv.org/abs/2301.10241","keyPoints":"Uses (d choose 2) planes to represent d-dimensional scenes; Natural space-time decomposition; 1000x compression over full 4D grid","keyHypotheses":"Planar factorization provides interpretable and efficient representation; Space-time decomposition enables dimension-specific priors","strengths":"Highly interpretable; Efficient factorization; Fast optimization","weaknesses":"Limited to planar factorization; May not capture all scene complexities","citation":"Fridovich-Keil, S., et al. (2023). K-planes: Explicit radiance fields in space, time, and appearance. CVPR.","notes":"Important for understanding planar factorization - relevant to 2D matrix decomposition","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"candes2009matrix","title":"Exact Matrix Completion via Convex Optimization","authors":"Emmanuel J. Candès, Benjamin Recht","journal":"Communications of the ACM","year":"2009","doi":"","url":"https://arxiv.org/abs/0903.1476","keyPoints":"Nuclear norm minimization for exact matrix recovery; Incoherence conditions for recovery guarantee; Convex relaxation of rank minimization","keyHypotheses":"Low-rank matrices can be exactly recovered from few entries via convex optimization; Nuclear norm is effective relaxation of rank function","strengths":"Theoretical guarantees; Convex optimization; Widely applicable","weaknesses":"Incoherence conditions may be restrictive; Discrete representation only","citation":"Candès, E. J., & Recht, B. (2009). Exact matrix completion via convex optimization. Communications of the ACM.","notes":"Fundamental matrix completion theory - baseline for comparison with INR methods","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"recht2011simpler","title":"A Simpler Approach to Matrix Completion","authors":"Benjamin Recht","journal":"Journal of Machine Learning Research","year":"2011","doi":"","url":"https://jmlr.org/papers/v12/recht11a.html","keyPoints":"Improved bounds on entries required for matrix completion; Elementary proof using quantum information theory; Nuclear norm minimization","keyHypotheses":"Tighter sample complexity bounds achievable with simpler analysis; Random sampling is sufficient for recovery","strengths":"Tighter theoretical bounds; Simpler proofs; Elementary analysis","weaknesses":"Still requires incoherence conditions; Limited to low-rank case","citation":"Recht, B. (2011). A simpler approach to matrix completion. Journal of Machine Learning Research, 12, 3413-3430.","notes":"Improved theoretical foundation for matrix completion","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"shi2024inr","title":"Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction","authors":"Jiayang Shi, Junyi Zhu, Daniel M. Pelt, K. Joost Batenburg, Matthew B. Blaschko","journal":"Transactions on Machine Learning Research","year":"2024","doi":"","url":"https://arxiv.org/abs/2405.02509","keyPoints":"Joint reconstruction using INRs for multiple objects; Bayesian framework with latent variables; Common patterns assist individual reconstruction","keyHypotheses":"Joint learning across multiple objects improves individual reconstruction quality; Common patterns can be captured via latent variables","strengths":"Improved reconstruction quality; Robust to noise; Novel joint learning approach","weaknesses":"Requires multiple related objects; More complex optimization","citation":"Shi, J., et al. (2024). Implicit neural representations for robust joint sparse-view CT reconstruction. TMLR.","notes":"Shows how INRs can be applied to reconstruction problems with sparse observations","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"kim2025grids","title":"Grids Often Outperform Implicit Neural Representations","authors":"Namhoon Kim, Sara Fridovich-Keil","journal":"EESS","year":"2025","doi":"10.48550/arXiv.2506.11139","url":"https://arxiv.org/abs/2506.11139","keyPoints":"Systematic comparison of INRs vs grids across 2D/3D tasks; Regularized grids with interpolation train faster; INRs excel only for lower-dimensional structure signals","keyHypotheses":"Simple regularized grids outperform INRs for most reconstruction tasks; INRs advantage limited to signals with underlying structure","strengths":"Comprehensive evaluation; Clear performance boundaries; Practical guidance for method selection","weaknesses":"Limited to specific signal types; May not generalize to all INR variants","citation":"Kim, N., & Fridovich-Keil, S. (2025). Grids Often Outperform Implicit Neural Representations. EESS.","notes":"NEAREST NEIGHBOR PAPER - Direct comparison of approaches most relevant to our matrix reconstruction work","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"kerbl2023gaussian","title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering","authors":"Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis","journal":"SIGGRAPH","year":"2023","doi":"","url":"https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/","keyPoints":"3D Gaussians for scene representation; Real-time rendering (≥30 fps at 1080p); Interleaved optimization with density control","keyHypotheses":"Explicit 3D Gaussians can achieve real-time quality; Anisotropic covariance enables accurate scene representation","strengths":"Real-time performance; High visual quality; Efficient representation","weaknesses":"Limited to 3D radiance fields; Memory intensive for complex scenes","citation":"Kerbl, B., et al. (2023). 3D Gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (SIGGRAPH).","notes":"Revolutionary explicit representation alternative to INRs - relevant for explicit vs implicit comparisons","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"mueller2022instant","title":"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding","authors":"Thomas Müller, Alex Evans, Christoph Schied, Alex Keller","journal":"SIGGRAPH","year":"2022","doi":"","url":"https://research.nvidia.com/publication/2022-07_instant-neural-graphics-primitives-multiresolution-hash-encoding","keyPoints":"Multiresolution hash table encoding; Orders of magnitude speedup; Small network architecture with hash features","keyHypotheses":"Hash encoding can replace large MLPs; Multiresolution structure disambiguates collisions","strengths":"Massive speedup; Memory efficient; General framework","weaknesses":"Hash collision handling; Implementation complexity","citation":"Müller, T., et al. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (SIGGRAPH).","notes":"Best Technical Paper SIGGRAPH 2022 - encoding breakthrough relevant to all INR methods","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"yu2022plenoxels","title":"Plenoxels: Radiance Fields without Neural Networks","authors":"Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa","journal":"CVPR","year":"2022","doi":"","url":"https://ieeexplore.ieee.org/document/9880358/","keyPoints":"Sparse 3D grid with spherical harmonics; 100x faster optimization than NeRF; No neural networks required","keyHypotheses":"Explicit sparse grids can match NeRF quality; Neural networks unnecessary for radiance field representation","strengths":"Dramatic speedup; Simple optimization; No neural components","weaknesses":"Memory scaling issues; Limited expressiveness vs neural methods","citation":"Yu, A., et al. (2022). Plenoxels: Radiance fields without neural networks. CVPR.","notes":"Demonstrates explicit representations can match neural quality - highly relevant to grid vs INR comparison","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"vyas2024strainer","title":"Learning Transferable Features for Implicit Neural Representations","authors":"Kushal Vyas, Ahmed Imtiaz Humayun, Aniket Dashpute, Richard G. Baraniuk, Ashok Veeraraghavan, Guha Balakrishnan","journal":"NeurIPS","year":"2024","doi":"","url":"https://proceedings.neurips.cc/paper_files/paper/2024/hash/4a8bc86ca475c229dc1fd0f4d5cf8f63-Abstract-Conference.html","keyPoints":"Shared encoder layers across multiple INRs; +10dB signal quality improvement; Transfer learning for INRs","keyHypotheses":"INR features can be made transferable; Shared representations accelerate convergence","strengths":"Significant quality improvement; Transfer learning capability; Modular architecture","weaknesses":"Domain-specific training; Additional complexity","citation":"Vyas, K., et al. (2024). Learning transferable features for implicit neural representations. NeurIPS.","notes":"Shows how to improve INR efficiency through transfer learning - relevant to matrix reconstruction acceleration","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"wang2025metricgrids","title":"MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation","authors":"Shu Wang, Yanbo Gao, Shuai Li, Chong Lv, Xun Cai, Chuankun Li, Hui Yuan, Jinglin Zhang","journal":"CVPR","year":"2025","doi":"","url":"https://arxiv.org/abs/2503.10000","keyPoints":"Multiple elementary metric grids; High-order terms via Taylor expansion; Hash encoding with different sparsities","keyHypotheses":"Metric grids in different spaces can capture nonlinearities; High-order terms improve approximation quality","strengths":"Novel grid structure; Strong theoretical foundation; Superior fitting accuracy","weaknesses":"Increased complexity; Memory requirements","citation":"Wang, S., et al. (2025). MetricGrids: Arbitrary nonlinear approximation with elementary metric grids based implicit neural representation. CVPR.","notes":"Novel grid-based INR approach - bridges explicit and implicit methods","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"essakine2024survey","title":"Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey","authors":"Amer Essakine, Yanqi Cheng, Chun-Wun Cheng, Lipei Zhang, Zhongying Deng, Lei Zhu, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero","journal":"arXiv","year":"2024","doi":"","url":"https://arxiv.org/abs/2411.03688","keyPoints":"Comprehensive INR survey with clear taxonomy; Resolution independence and memory efficiency; Applications in audio, image, 3D reconstruction; Complex inverse problem solving capability","keyHypotheses":"INRs offer exceptional flexibility across diverse applications; Continuous functions provide advantages over discretized structures","strengths":"Comprehensive coverage; Clear categorization; Performance analysis across domains","weaknesses":"Survey paper - no novel methods","citation":"Essakine, A., et al. (2024). Where do we stand with implicit neural representations? A technical and performance survey. arXiv.","notes":"Major 2024 survey establishing current state of INR field","addedDate":"2025-10-07T00:18:00.000Z"}
{"id":"cheng2025lowrank","title":"Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization","authors":"Zhengyun Cheng, Changhao Wang, Guanwen Zhang, Yi Xu, Wei Zhou, Xiangyang Ji","journal":"arXiv","year":"2025","doi":"","url":"https://arxiv.org/abs/2506.22134","keyPoints":"CP-based low-rank tensor function with neural networks; Variational Schatten-p quasi-norm for sparse solutions; Theoretical guarantees on excess risk bounds","keyHypotheses":"CP decomposition provides more interpretable tensor structure; Schatten-p quasi-norm enables sparse solutions; Jacobian regularization improves smoothness","strengths":"Strong theoretical foundation; Direct tensor-INR connection; Sparse solutions","weaknesses":"Increased mathematical complexity; Limited empirical evaluation","citation":"Cheng, Z., et al. (2025). Low-rank implicit neural representation via Schatten-p quasi-norm and Jacobian regularization. arXiv.","notes":"Direct connection between low-rank tensor methods and INRs - highly relevant to matrix reconstruction","addedDate":"2025-10-07T00:18:00.000Z"}
{"id":"razin2024rank","title":"Understanding Deep Learning via Notions of Rank","authors":"Noam Razin","journal":"arXiv (PhD Thesis)","year":"2024","doi":"","url":"https://arxiv.org/abs/2408.02111","keyPoints":"Rank as key for deep learning theory; Gradient training induces low-rank regularization; Connection between neural networks and tensor factorizations; Explains generalization on natural data","keyHypotheses":"Low-rank bias facilitates generalization; Rank measures are fundamental to understanding expressiveness; Tensor connections enable theoretical analysis","strengths":"Comprehensive theoretical framework; Connects multiple domains; Practical implications","weaknesses":"Theory-focused; Limited empirical validation","citation":"Razin, N. (2024). Understanding deep learning via notions of rank. arXiv (PhD Thesis).","notes":"Foundational theoretical work connecting rank, neural networks, and tensor factorizations","addedDate":"2025-10-07T00:18:00.000Z"}
{"id":"borsoi2024tensor","title":"Low-Rank Tensor Decompositions for the Theory of Neural Networks","authors":"Ricardo Borsoi, Konstantin Usevich, Marianne Clausel","journal":"arXiv","year":"2024","doi":"","url":"https://arxiv.org/abs/2508.18408","keyPoints":"Unified framework using tensor decompositions for NN theory; Explains expressivity, learnability, generalization, identifiability; Strong uniqueness guarantees; Polynomial-time algorithms","keyHypotheses":"Tensor methods fundamental to understanding NN performance; Low-rank structure key to NN success; Theoretical results support advances","strengths":"Comprehensive theoretical review; Connects multiple communities; Strong mathematical foundation","weaknesses":"Review/survey nature; No novel empirical results","citation":"Borsoi, R., et al. (2024). Low-rank tensor decompositions for the theory of neural networks. arXiv.","notes":"Major theoretical review connecting tensor methods to neural network understanding","addedDate":"2025-10-07T00:18:00.000Z"}
{"id":"hamreras2025tensorization","title":"Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks","authors":"Safa Hamreras, Sukhbinder Singh, Román Orús","journal":"arXiv","year":"2025","doi":"","url":"https://arxiv.org/abs/2505.20132","keyPoints":"Position paper advocating tensor network adoption; Bond indices create new latent spaces; Mechanistic interpretability through tensor structure; Distinctive scaling properties","keyHypotheses":"TNNs deserve greater attention; Bond indices enable deeper insight; Interpretability advances through tensor structure","strengths":"Strong advocacy for underexplored area; Identifies interpretability benefits; Cross-domain collaboration","weaknesses":"Position paper - limited empirical content; Theoretical focus","citation":"Hamreras, S., et al. (2025). Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks. arXiv.","notes":"Position paper arguing for wider tensor method adoption in deep learning","addedDate":"2025-10-07T00:18:00.000Z"}
{"id":"wu2024gaussiansplat","title":"Recent Advances in 3D Gaussian Splatting","authors":"Tong Wu, Yu-Jie Yuan, Ling-Xiao Zhang, Jie Yang, Yan-Pei Cao, Ling-Qi Yan, Lin Gao","journal":"Computational Visual Media (Springer)","year":"2024","doi":"","url":"https://arxiv.org/abs/2403.11134","keyPoints":"Comprehensive 3DGS survey; Classification by functionality; Point-based rendering to modern 3DGS; Explicit representation advantages","keyHypotheses":"Explicit representations enable interpretability; Rasterization achieves real-time performance; Direct manipulation valuable","strengths":"Comprehensive coverage; Clear classification; Performance analysis","weaknesses":"Survey paper; Limited to 3D domain","citation":"Wu, T., et al. (2024). Recent advances in 3D Gaussian splatting. Computational Visual Media.","notes":"Major survey of 3D Gaussian splatting - explicit vs implicit insights relevant to matrix reconstruction","addedDate":"2025-10-07T00:18:00.000Z"}
{"id":"chandravamsi2024winner","title":"Improving Accuracy and Efficiency of Implicit Neural Representations: Making SIREN a WINNER","authors":"Hemanth Chandravamsi, Dhanush V. Shenoy, Steven H. Frankel","journal":"arXiv","year":"2024","doi":"","url":"https://arxiv.org/abs/2509.12980","keyPoints":"WINNER addresses SIREN spectral bottleneck; Adaptive Gaussian noise based on spectral centroid; State-of-the-art audio fitting; Target-aware initialization","keyHypotheses":"Spectral misalignment causes SIREN failure; Adaptive noise injection mitigates spectral bias; Target-aware initialization improves performance","strengths":"Addresses fundamental SIREN limitation; No additional parameters; Strong empirical results","weaknesses":"Limited to SIREN networks; Requires spectral analysis","citation":"Chandravamsi, H., et al. (2024). Improving accuracy and efficiency of implicit neural representations: Making SIREN a WINNER. arXiv.","notes":"Addresses SIREN limitations through adaptive initialization - relevant to matrix frequency analysis","addedDate":"2025-10-07T00:18:00.000Z"}
{"id":"niemeyer2024radsplat","title":"RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS","authors":"Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, et al.","journal":"arXiv (Google)","year":"2024","doi":"","url":"https://arxiv.org/abs/2403.13806","keyPoints":"Hybrid radiance field and Gaussian splatting; 900+ FPS real-time rendering; Robust initialization for challenging scenes; Best of both worlds approach","keyHypotheses":"Combining initialization approaches improves robustness; Hybrid methods outperform pure approaches; Real-time quality achievable","strengths":"Exceptional performance; Hybrid approach; Real-time rendering","weaknesses":"Complex implementation; Limited to 3D scenes","citation":"Niemeyer, M., et al. (2024). RadSplat: Radiance field-informed Gaussian splatting for robust real-time rendering with 900+ FPS. arXiv.","notes":"Exemplary hybrid explicit-implicit approach - relevant to matrix reconstruction hybrid methods","addedDate":"2025-10-07T00:18:00.000Z"}
{"id":"ruszwurm2024geographic","title":"Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks","authors":"Marc Rußwurm, Konstantin Klemmer, Esther Rolf, Robin Zbinden, Devis Tuia","journal":"ICLR","year":"2024","doi":"","url":"https://openreview.net/forum?id=PudduufFLa","keyPoints":"Spherical harmonic basis functions for geographic data; Eliminates pole artifacts from rectangular assumptions; SIREN integration; Global coordinate system","keyHypotheses":"Native spherical representations better than rectangular; Proper coordinate systems matter; Basis functions should match data characteristics","strengths":"Domain-specific solution; Eliminates artifacts; Strong empirical results","weaknesses":"Limited to geographic applications; Specialized domain","citation":"Rußwurm, M., et al. (2024). Geographic location encoding with spherical harmonics and sinusoidal representation networks. ICLR Spotlight.","notes":"Domain-specific positional encoding - demonstrates importance of matching encoding to data characteristics","addedDate":"2025-10-07T00:18:00.000Z"}
