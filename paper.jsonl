{"id":"tancik2020fourier","title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains","authors":"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.10739","keyPoints":"Fourier feature mapping enables MLPs to learn high-frequency functions; Overcomes spectral bias using Neural Tangent Kernel theory; Simple mapping: γ(v) = [cos(2πBv), sin(2πBv)]^T","keyHypotheses":"Standard MLPs fail at high frequencies due to spectral bias; Fourier features can transform NTK into stationary kernel with tunable bandwidth","strengths":"Theoretical foundation via NTK; Simple and effective method; Wide applicability to computer vision tasks","weaknesses":"Requires careful frequency selection; May not capture all signal patterns","citation":"Tancik, M., et al. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS.","notes":"Seminal work for positional encoding in INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"sitzmann2020siren","title":"Implicit Neural Representations with Periodic Activation Functions","authors":"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.09661","keyPoints":"Uses sine activation functions for representing complex signals; All derivatives of sine are sine functions; Special initialization scheme for stable training","keyHypotheses":"Periodic activations better represent natural signals than ReLU; Derivative access enables solving differential equations","strengths":"Smooth representations; Derivative access at any order; Works across multiple domains","weaknesses":"Sensitive to initialization; Limited theoretical analysis","citation":"Sitzmann, V., et al. (2020). Implicit neural representations with periodic activation functions. NeurIPS.","notes":"Alternative to Fourier features for INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"mildenhall2020nerf","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","authors":"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng","journal":"ECCV","year":"2020","doi":"","url":"https://arxiv.org/abs/2003.08934","keyPoints":"Represents 3D scenes as continuous 5D radiance fields; Uses positional encoding and volume rendering; Hierarchical sampling with coarse and fine networks","keyHypotheses":"Scenes can be represented as continuous functions mapping coordinates to color and density; MLPs with positional encoding can capture complex 3D structure","strengths":"Revolutionary 3D representation; High-quality novel view synthesis; Continuous representation","weaknesses":"Slow training and inference; Requires many input views","citation":"Mildenhall, B., et al. (2020). NeRF: Representing scenes as neural radiance fields for view synthesis. ECCV.","notes":"Foundational work showing power of INRs for 3D reconstruction","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"chen2022tensorf","title":"TensoRF: Tensorial Radiance Fields","authors":"Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su","journal":"ECCV","year":"2022","doi":"","url":"https://arxiv.org/abs/2203.09517","keyPoints":"Models radiance field as 4D tensor with factorization; CP decomposition and Vector-Matrix decomposition; Fast reconstruction with compact models","keyHypotheses":"Tensor factorization can accelerate INR training while maintaining quality; Low-rank structure exists in radiance fields","strengths":"Significant speedup over NeRF; Compact model size; Multiple factorization options","weaknesses":"Still requires careful hyperparameter tuning; Limited to radiance field domain","citation":"Chen, A., et al. (2022). TensoRF: Tensorial radiance fields. ECCV.","notes":"Key paper for tensor factorization in neural fields - directly relevant to matrix factorization","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"fridovich2023kplanes","title":"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance","authors":"Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, Angjoo Kanazawa","journal":"CVPR","year":"2023","doi":"","url":"https://arxiv.org/abs/2301.10241","keyPoints":"Uses (d choose 2) planes to represent d-dimensional scenes; Natural space-time decomposition; 1000x compression over full 4D grid","keyHypotheses":"Planar factorization provides interpretable and efficient representation; Space-time decomposition enables dimension-specific priors","strengths":"Highly interpretable; Efficient factorization; Fast optimization","weaknesses":"Limited to planar factorization; May not capture all scene complexities","citation":"Fridovich-Keil, S., et al. (2023). K-planes: Explicit radiance fields in space, time, and appearance. CVPR.","notes":"Important for understanding planar factorization - relevant to 2D matrix decomposition","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"candes2009matrix","title":"Exact Matrix Completion via Convex Optimization","authors":"Emmanuel J. Candès, Benjamin Recht","journal":"Communications of the ACM","year":"2009","doi":"","url":"https://arxiv.org/abs/0903.1476","keyPoints":"Nuclear norm minimization for exact matrix recovery; Incoherence conditions for recovery guarantee; Convex relaxation of rank minimization","keyHypotheses":"Low-rank matrices can be exactly recovered from few entries via convex optimization; Nuclear norm is effective relaxation of rank function","strengths":"Theoretical guarantees; Convex optimization; Widely applicable","weaknesses":"Incoherence conditions may be restrictive; Discrete representation only","citation":"Candès, E. J., & Recht, B. (2009). Exact matrix completion via convex optimization. Communications of the ACM.","notes":"Fundamental matrix completion theory - baseline for comparison with INR methods","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"recht2011simpler","title":"A Simpler Approach to Matrix Completion","authors":"Benjamin Recht","journal":"Journal of Machine Learning Research","year":"2011","doi":"","url":"https://jmlr.org/papers/v12/recht11a.html","keyPoints":"Improved bounds on entries required for matrix completion; Elementary proof using quantum information theory; Nuclear norm minimization","keyHypotheses":"Tighter sample complexity bounds achievable with simpler analysis; Random sampling is sufficient for recovery","strengths":"Tighter theoretical bounds; Simpler proofs; Elementary analysis","weaknesses":"Still requires incoherence conditions; Limited to low-rank case","citation":"Recht, B. (2011). A simpler approach to matrix completion. Journal of Machine Learning Research, 12, 3413-3430.","notes":"Improved theoretical foundation for matrix completion","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"shi2024inr","title":"Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction","authors":"Jiayang Shi, Junyi Zhu, Daniel M. Pelt, K. Joost Batenburg, Matthew B. Blaschko","journal":"Transactions on Machine Learning Research","year":"2024","doi":"","url":"https://arxiv.org/abs/2405.02509","keyPoints":"Joint reconstruction using INRs for multiple objects; Bayesian framework with latent variables; Common patterns assist individual reconstruction","keyHypotheses":"Joint learning across multiple objects improves individual reconstruction quality; Common patterns can be captured via latent variables","strengths":"Improved reconstruction quality; Robust to noise; Novel joint learning approach","weaknesses":"Requires multiple related objects; More complex optimization","citation":"Shi, J., et al. (2024). Implicit neural representations for robust joint sparse-view CT reconstruction. TMLR.","notes":"Shows how INRs can be applied to reconstruction problems with sparse observations","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"kim2025grids","title":"Grids Often Outperform Implicit Neural Representations","authors":"Namhoon Kim, Sara Fridovich-Keil","journal":"EESS","year":"2025","doi":"10.48550/arXiv.2506.11139","url":"https://arxiv.org/abs/2506.11139","keyPoints":"Systematic comparison of INRs vs grids across 2D/3D tasks; Regularized grids with interpolation train faster; INRs excel only for lower-dimensional structure signals","keyHypotheses":"Simple regularized grids outperform INRs for most reconstruction tasks; INRs advantage limited to signals with underlying structure","strengths":"Comprehensive evaluation; Clear performance boundaries; Practical guidance for method selection","weaknesses":"Limited to specific signal types; May not generalize to all INR variants","citation":"Kim, N., & Fridovich-Keil, S. (2025). Grids Often Outperform Implicit Neural Representations. EESS.","notes":"NEAREST NEIGHBOR PAPER - Direct comparison of approaches most relevant to our matrix reconstruction work","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"kerbl2023gaussian","title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering","authors":"Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis","journal":"SIGGRAPH","year":"2023","doi":"","url":"https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/","keyPoints":"3D Gaussians for scene representation; Real-time rendering (≥30 fps at 1080p); Interleaved optimization with density control","keyHypotheses":"Explicit 3D Gaussians can achieve real-time quality; Anisotropic covariance enables accurate scene representation","strengths":"Real-time performance; High visual quality; Efficient representation","weaknesses":"Limited to 3D radiance fields; Memory intensive for complex scenes","citation":"Kerbl, B., et al. (2023). 3D Gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (SIGGRAPH).","notes":"Revolutionary explicit representation alternative to INRs - relevant for explicit vs implicit comparisons","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"mueller2022instant","title":"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding","authors":"Thomas Müller, Alex Evans, Christoph Schied, Alex Keller","journal":"SIGGRAPH","year":"2022","doi":"","url":"https://research.nvidia.com/publication/2022-07_instant-neural-graphics-primitives-multiresolution-hash-encoding","keyPoints":"Multiresolution hash table encoding; Orders of magnitude speedup; Small network architecture with hash features","keyHypotheses":"Hash encoding can replace large MLPs; Multiresolution structure disambiguates collisions","strengths":"Massive speedup; Memory efficient; General framework","weaknesses":"Hash collision handling; Implementation complexity","citation":"Müller, T., et al. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (SIGGRAPH).","notes":"Best Technical Paper SIGGRAPH 2022 - encoding breakthrough relevant to all INR methods","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"yu2022plenoxels","title":"Plenoxels: Radiance Fields without Neural Networks","authors":"Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa","journal":"CVPR","year":"2022","doi":"","url":"https://ieeexplore.ieee.org/document/9880358/","keyPoints":"Sparse 3D grid with spherical harmonics; 100x faster optimization than NeRF; No neural networks required","keyHypotheses":"Explicit sparse grids can match NeRF quality; Neural networks unnecessary for radiance field representation","strengths":"Dramatic speedup; Simple optimization; No neural components","weaknesses":"Memory scaling issues; Limited expressiveness vs neural methods","citation":"Yu, A., et al. (2022). Plenoxels: Radiance fields without neural networks. CVPR.","notes":"Demonstrates explicit representations can match neural quality - highly relevant to grid vs INR comparison","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"vyas2024strainer","title":"Learning Transferable Features for Implicit Neural Representations","authors":"Kushal Vyas, Ahmed Imtiaz Humayun, Aniket Dashpute, Richard G. Baraniuk, Ashok Veeraraghavan, Guha Balakrishnan","journal":"NeurIPS","year":"2024","doi":"","url":"https://proceedings.neurips.cc/paper_files/paper/2024/hash/4a8bc86ca475c229dc1fd0f4d5cf8f63-Abstract-Conference.html","keyPoints":"Shared encoder layers across multiple INRs; +10dB signal quality improvement; Transfer learning for INRs","keyHypotheses":"INR features can be made transferable; Shared representations accelerate convergence","strengths":"Significant quality improvement; Transfer learning capability; Modular architecture","weaknesses":"Domain-specific training; Additional complexity","citation":"Vyas, K., et al. (2024). Learning transferable features for implicit neural representations. NeurIPS.","notes":"Shows how to improve INR efficiency through transfer learning - relevant to matrix reconstruction acceleration","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"wang2025metricgrids","title":"MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation","authors":"Shu Wang, Yanbo Gao, Shuai Li, Chong Lv, Xun Cai, Chuankun Li, Hui Yuan, Jinglin Zhang","journal":"CVPR","year":"2025","doi":"","url":"https://arxiv.org/abs/2503.10000","keyPoints":"Multiple elementary metric grids; High-order terms via Taylor expansion; Hash encoding with different sparsities","keyHypotheses":"Metric grids in different spaces can capture nonlinearities; High-order terms improve approximation quality","strengths":"Novel grid structure; Strong theoretical foundation; Superior fitting accuracy","weaknesses":"Increased complexity; Memory requirements","citation":"Wang, S., et al. (2025). MetricGrids: Arbitrary nonlinear approximation with elementary metric grids based implicit neural representation. CVPR.","notes":"Novel grid-based INR approach - bridges explicit and implicit methods","addedDate":"2025-09-15T03:36:00.000Z"}
