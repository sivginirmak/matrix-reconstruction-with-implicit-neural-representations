{"id":"tancik2020fourier","title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains","authors":"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.10739","keyPoints":"Fourier feature mapping enables MLPs to learn high-frequency functions; Overcomes spectral bias using Neural Tangent Kernel theory; Simple mapping: γ(v) = [cos(2πBv), sin(2πBv)]^T","keyHypotheses":"Standard MLPs fail at high frequencies due to spectral bias; Fourier features can transform NTK into stationary kernel with tunable bandwidth","strengths":"Theoretical foundation via NTK; Simple and effective method; Wide applicability to computer vision tasks","weaknesses":"Requires careful frequency selection; May not capture all signal patterns","citation":"Tancik, M., et al. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS.","notes":"Seminal work for positional encoding in INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"sitzmann2020siren","title":"Implicit Neural Representations with Periodic Activation Functions","authors":"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.09661","keyPoints":"Uses sine activation functions for representing complex signals; All derivatives of sine are sine functions; Special initialization scheme for stable training","keyHypotheses":"Periodic activations better represent natural signals than ReLU; Derivative access enables solving differential equations","strengths":"Smooth representations; Derivative access at any order; Works across multiple domains","weaknesses":"Sensitive to initialization; Limited theoretical analysis","citation":"Sitzmann, V., et al. (2020). Implicit neural representations with periodic activation functions. NeurIPS.","notes":"Alternative to Fourier features for INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"mildenhall2020nerf","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","authors":"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng","journal":"ECCV","year":"2020","doi":"","url":"https://arxiv.org/abs/2003.08934","keyPoints":"Represents 3D scenes as continuous 5D radiance fields; Uses positional encoding and volume rendering; Hierarchical sampling with coarse and fine networks","keyHypotheses":"Scenes can be represented as continuous functions mapping coordinates to color and density; MLPs with positional encoding can capture complex 3D structure","strengths":"Revolutionary 3D representation; High-quality novel view synthesis; Continuous representation","weaknesses":"Slow training and inference; Requires many input views","citation":"Mildenhall, B., et al. (2020). NeRF: Representing scenes as neural radiance fields for view synthesis. ECCV.","notes":"Foundational work showing power of INRs for 3D reconstruction","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"chen2022tensorf","title":"TensoRF: Tensorial Radiance Fields","authors":"Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su","journal":"ECCV","year":"2022","doi":"","url":"https://arxiv.org/abs/2203.09517","keyPoints":"Models radiance field as 4D tensor with factorization; CP decomposition and Vector-Matrix decomposition; Fast reconstruction with compact models","keyHypotheses":"Tensor factorization can accelerate INR training while maintaining quality; Low-rank structure exists in radiance fields","strengths":"Significant speedup over NeRF; Compact model size; Multiple factorization options","weaknesses":"Still requires careful hyperparameter tuning; Limited to radiance field domain","citation":"Chen, A., et al. (2022). TensoRF: Tensorial radiance fields. ECCV.","notes":"Key paper for tensor factorization in neural fields - directly relevant to matrix factorization","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"fridovich2023kplanes","title":"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance","authors":"Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, Angjoo Kanazawa","journal":"CVPR","year":"2023","doi":"","url":"https://arxiv.org/abs/2301.10241","keyPoints":"Uses (d choose 2) planes to represent d-dimensional scenes; Natural space-time decomposition; 1000x compression over full 4D grid","keyHypotheses":"Planar factorization provides interpretable and efficient representation; Space-time decomposition enables dimension-specific priors","strengths":"Highly interpretable; Efficient factorization; Fast optimization","weaknesses":"Limited to planar factorization; May not capture all scene complexities","citation":"Fridovich-Keil, S., et al. (2023). K-planes: Explicit radiance fields in space, time, and appearance. CVPR.","notes":"Important for understanding planar factorization - relevant to 2D matrix decomposition","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"candes2009matrix","title":"Exact Matrix Completion via Convex Optimization","authors":"Emmanuel J. Candès, Benjamin Recht","journal":"Communications of the ACM","year":"2009","doi":"","url":"https://arxiv.org/abs/0903.1476","keyPoints":"Nuclear norm minimization for exact matrix recovery; Incoherence conditions for recovery guarantee; Convex relaxation of rank minimization","keyHypotheses":"Low-rank matrices can be exactly recovered from few entries via convex optimization; Nuclear norm is effective relaxation of rank function","strengths":"Theoretical guarantees; Convex optimization; Widely applicable","weaknesses":"Incoherence conditions may be restrictive; Discrete representation only","citation":"Candès, E. J., & Recht, B. (2009). Exact matrix completion via convex optimization. Communications of the ACM.","notes":"Fundamental matrix completion theory - baseline for comparison with INR methods","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"recht2011simpler","title":"A Simpler Approach to Matrix Completion","authors":"Benjamin Recht","journal":"Journal of Machine Learning Research","year":"2011","doi":"","url":"https://jmlr.org/papers/v12/recht11a.html","keyPoints":"Improved bounds on entries required for matrix completion; Elementary proof using quantum information theory; Nuclear norm minimization","keyHypotheses":"Tighter sample complexity bounds achievable with simpler analysis; Random sampling is sufficient for recovery","strengths":"Tighter theoretical bounds; Simpler proofs; Elementary analysis","weaknesses":"Still requires incoherence conditions; Limited to low-rank case","citation":"Recht, B. (2011). A simpler approach to matrix completion. Journal of Machine Learning Research, 12, 3413-3430.","notes":"Improved theoretical foundation for matrix completion","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"shi2024inr","title":"Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction","authors":"Jiayang Shi, Junyi Zhu, Daniel M. Pelt, K. Joost Batenburg, Matthew B. Blaschko","journal":"Transactions on Machine Learning Research","year":"2024","doi":"","url":"https://arxiv.org/abs/2405.02509","keyPoints":"Joint reconstruction using INRs for multiple objects; Bayesian framework with latent variables; Common patterns assist individual reconstruction","keyHypotheses":"Joint learning across multiple objects improves individual reconstruction quality; Common patterns can be captured via latent variables","strengths":"Improved reconstruction quality; Robust to noise; Novel joint learning approach","weaknesses":"Requires multiple related objects; More complex optimization","citation":"Shi, J., et al. (2024). Implicit neural representations for robust joint sparse-view CT reconstruction. TMLR.","notes":"Shows how INRs can be applied to reconstruction problems with sparse observations","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"kim2025grids","title":"Grids Often Outperform Implicit Neural Representations","authors":"Namhoon Kim, Sara Fridovich-Keil","journal":"EESS","year":"2025","doi":"10.48550/arXiv.2506.11139","url":"https://arxiv.org/abs/2506.11139","keyPoints":"Systematic comparison of INRs vs grids across 2D/3D tasks; Regularized grids with interpolation train faster; INRs excel only for lower-dimensional structure signals","keyHypotheses":"Simple regularized grids outperform INRs for most reconstruction tasks; INRs advantage limited to signals with underlying structure","strengths":"Comprehensive evaluation; Clear performance boundaries; Practical guidance for method selection","weaknesses":"Limited to specific signal types; May not generalize to all INR variants","citation":"Kim, N., & Fridovich-Keil, S. (2025). Grids Often Outperform Implicit Neural Representations. EESS.","notes":"NEAREST NEIGHBOR PAPER - Direct comparison of approaches most relevant to our matrix reconstruction work","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"kerbl2023gaussian","title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering","authors":"Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis","journal":"SIGGRAPH","year":"2023","doi":"","url":"https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/","keyPoints":"3D Gaussians for scene representation; Real-time rendering (≥30 fps at 1080p); Interleaved optimization with density control","keyHypotheses":"Explicit 3D Gaussians can achieve real-time quality; Anisotropic covariance enables accurate scene representation","strengths":"Real-time performance; High visual quality; Efficient representation","weaknesses":"Limited to 3D radiance fields; Memory intensive for complex scenes","citation":"Kerbl, B., et al. (2023). 3D Gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (SIGGRAPH).","notes":"Revolutionary explicit representation alternative to INRs - relevant for explicit vs implicit comparisons","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"mueller2022instant","title":"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding","authors":"Thomas Müller, Alex Evans, Christoph Schied, Alex Keller","journal":"SIGGRAPH","year":"2022","doi":"","url":"https://research.nvidia.com/publication/2022-07_instant-neural-graphics-primitives-multiresolution-hash-encoding","keyPoints":"Multiresolution hash table encoding; Orders of magnitude speedup; Small network architecture with hash features","keyHypotheses":"Hash encoding can replace large MLPs; Multiresolution structure disambiguates collisions","strengths":"Massive speedup; Memory efficient; General framework","weaknesses":"Hash collision handling; Implementation complexity","citation":"Müller, T., et al. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (SIGGRAPH).","notes":"Best Technical Paper SIGGRAPH 2022 - encoding breakthrough relevant to all INR methods","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"yu2022plenoxels","title":"Plenoxels: Radiance Fields without Neural Networks","authors":"Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa","journal":"CVPR","year":"2022","doi":"","url":"https://ieeexplore.ieee.org/document/9880358/","keyPoints":"Sparse 3D grid with spherical harmonics; 100x faster optimization than NeRF; No neural networks required","keyHypotheses":"Explicit sparse grids can match NeRF quality; Neural networks unnecessary for radiance field representation","strengths":"Dramatic speedup; Simple optimization; No neural components","weaknesses":"Memory scaling issues; Limited expressiveness vs neural methods","citation":"Yu, A., et al. (2022). Plenoxels: Radiance fields without neural networks. CVPR.","notes":"Demonstrates explicit representations can match neural quality - highly relevant to grid vs INR comparison","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"vyas2024strainer","title":"Learning Transferable Features for Implicit Neural Representations","authors":"Kushal Vyas, Ahmed Imtiaz Humayun, Aniket Dashpute, Richard G. Baraniuk, Ashok Veeraraghavan, Guha Balakrishnan","journal":"NeurIPS","year":"2024","doi":"","url":"https://proceedings.neurips.cc/paper_files/paper/2024/hash/4a8bc86ca475c229dc1fd0f4d5cf8f63-Abstract-Conference.html","keyPoints":"Shared encoder layers across multiple INRs; +10dB signal quality improvement; Transfer learning for INRs","keyHypotheses":"INR features can be made transferable; Shared representations accelerate convergence","strengths":"Significant quality improvement; Transfer learning capability; Modular architecture","weaknesses":"Domain-specific training; Additional complexity","citation":"Vyas, K., et al. (2024). Learning transferable features for implicit neural representations. NeurIPS.","notes":"Shows how to improve INR efficiency through transfer learning - relevant to matrix reconstruction acceleration","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"wang2025metricgrids","title":"MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation","authors":"Shu Wang, Yanbo Gao, Shuai Li, Chong Lv, Xun Cai, Chuankun Li, Hui Yuan, Jinglin Zhang","journal":"CVPR","year":"2025","doi":"","url":"https://arxiv.org/abs/2503.10000","keyPoints":"Multiple elementary metric grids; High-order terms via Taylor expansion; Hash encoding with different sparsities","keyHypotheses":"Metric grids in different spaces can capture nonlinearities; High-order terms improve approximation quality","strengths":"Novel grid structure; Strong theoretical foundation; Superior fitting accuracy","weaknesses":"Increased complexity; Memory requirements","citation":"Wang, S., et al. (2025). MetricGrids: Arbitrary nonlinear approximation with elementary metric grids based implicit neural representation. CVPR.","notes":"Novel grid-based INR approach - bridges explicit and implicit methods","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"barron2021mipnerf","title":"Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields","authors":"Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan","journal":"ICCV","year":"2021","doi":"","url":"https://arxiv.org/abs/2103.13415","keyPoints":"Multiscale representation using integrated positional encoding; Anti-aliasing through cone tracing; Improved rendering quality at different scales","keyHypotheses":"Multiscale representations can eliminate aliasing artifacts; Integrated positional encoding handles scale variations better","strengths":"Superior anti-aliasing; Better scale handling; Improved visual quality","weaknesses":"Increased computational complexity; Still requires hierarchical sampling","citation":"Barron, J. T., et al. (2021). Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields. ICCV.","notes":"Advances multiscale representations - relevant to matrix reconstruction at different resolutions","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"barron2022mipnerf360","title":"Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields","authors":"Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman","journal":"CVPR","year":"2022","doi":"","url":"https://arxiv.org/abs/2111.12077","keyPoints":"Unbounded scene representation; Online distillation for regularization; Improved background modeling with separate MLPs","keyHypotheses":"Separate foreground/background modeling improves unbounded scenes; Online distillation prevents overfitting","strengths":"Handles unbounded scenes; Improved regularization; Better background modeling","weaknesses":"More complex architecture; Higher memory requirements","citation":"Barron, J. T., et al. (2022). Mip-NeRF 360: Unbounded anti-aliased neural radiance fields. CVPR.","notes":"Advances in regularization and scene decomposition - relevant to matrix regularization techniques","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"chen2023factor","title":"Factor Fields: A Unified Framework for Neural Fields and Beyond","authors":"Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, Andreas Geiger","journal":"arXiv","year":"2023","doi":"","url":"https://arxiv.org/abs/2302.01226","keyPoints":"Unified framework for different neural field factorizations; CP, VM, and matrix factorization unified; Automatic rank selection and regularization","keyHypotheses":"Different factorization schemes can be unified under single framework; Automatic rank selection improves generalization","strengths":"General framework; Automatic hyperparameter selection; Multiple factorization support","weaknesses":"Framework complexity; Computational overhead for rank selection","citation":"Chen, A., et al. (2023). Factor fields: A unified framework for neural fields and beyond. arXiv.","notes":"Unifies tensor factorization approaches - directly relevant to matrix factorization framework design","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"rebain2021derf","title":"DeRF: Decomposed Radiance Fields","authors":"Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi","journal":"CVPR","year":"2021","doi":"","url":"https://arxiv.org/abs/2011.12490","keyPoints":"Spatial decomposition of radiance fields; Voronoi tessellation for scene partitioning; Independent optimization of sub-regions","keyHypotheses":"Spatial decomposition can accelerate training and improve quality; Independent optimization enables parallelization","strengths":"Faster training; Improved quality; Parallelizable optimization","weaknesses":"Boundary artifacts; Complex partitioning strategy","citation":"Rebain, D., et al. (2021). DeRF: Decomposed radiance fields. CVPR.","notes":"Demonstrates spatial decomposition techniques - relevant to matrix block decomposition strategies","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"martin2021nerf","title":"NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections","authors":"Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth","journal":"CVPR","year":"2021","doi":"","url":"https://arxiv.org/abs/2008.02268","keyPoints":"Handles illumination and transient objects; Appearance embedding for varying conditions; Separate static and transient components","keyHypotheses":"Appearance variations can be modeled with latent codes; Static/transient decomposition improves reconstruction","strengths":"Handles real-world variations; Robust to transients; Appearance modeling","weaknesses":"Requires many input images; Complex appearance model","citation":"Martin-Brualla, R., et al. (2021). NeRF in the wild: Neural radiance fields for unconstrained photo collections. CVPR.","notes":"Advances in handling data variations - relevant to matrix completion with missing patterns","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"lin2022efficientnerf","title":"EfficientNeRF: Efficient Neural Radiance Fields","authors":"Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, Jiaya Jia","journal":"CVPR","year":"2022","doi":"","url":"https://arxiv.org/abs/2206.00878","keyPoints":"Efficient sampling strategies; Early ray termination; Adaptive subdivision for importance sampling","keyHypotheses":"Smart sampling can dramatically reduce computation; Early termination maintains quality with fewer evaluations","strengths":"Significant speedup; Maintains quality; Practical efficiency gains","weaknesses":"Sampling complexity; Parameter tuning required","citation":"Hu, T., et al. (2022). EfficientNeRF: Efficient neural radiance fields. CVPR.","notes":"Efficiency advances relevant to accelerating matrix reconstruction inference","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"deng2022depth","title":"Depth-supervised NeRF: Fewer Views and Faster Training for Free","authors":"Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan","journal":"CVPR","year":"2022","doi":"","url":"https://arxiv.org/abs/2107.02791","keyPoints":"Depth supervision for better generalization; Fewer input views required; Faster convergence with depth priors","keyHypotheses":"Geometric priors accelerate training and improve generalization; Depth supervision reduces view requirements","strengths":"Fewer views needed; Faster training; Better generalization","weaknesses":"Requires depth data; Additional supervision complexity","citation":"Deng, K., et al. (2022). Depth-supervised NeRF: Fewer views and faster training for free. CVPR.","notes":"Shows how priors can improve INR efficiency - relevant to using structural priors in matrix completion","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"rosu2023permutonerf","title":"PermutoNeRF: Radiance Fields from Latent Permutohedra","authors":"Radu Alexandru Rosu, Sven Behnke","journal":"CVPR","year":"2023","doi":"","url":"https://arxiv.org/abs/2211.15599","keyPoints":"Lattice-based representation using permutohedra; Learnable lattice deformation; Fast training and rendering","keyHypotheses":"Lattice structures can efficiently represent radiance fields; Learnable deformation improves expressiveness","strengths":"Fast training; Efficient representation; Novel geometric approach","weaknesses":"Limited to specific lattice types; Complex geometric operations","citation":"Rosu, R. A., & Behnke, S. (2023). PermutoNeRF: Radiance fields from latent permutohedra. CVPR.","notes":"Novel geometric representation approach - provides alternative to grid-based methods","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"takikawa2022nglod","title":"Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes","authors":"Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler","journal":"CVPR","year":"2021","doi":"","url":"https://arxiv.org/abs/2101.10994","keyPoints":"Octree-based neural representation; Level-of-detail rendering; Real-time performance for implicit shapes","keyHypotheses":"Hierarchical representation enables real-time rendering; Level-of-detail maintains quality with efficiency","strengths":"Real-time performance; Hierarchical structure; Scalable representation","weaknesses":"Limited to specific geometric types; Complex hierarchical management","citation":"Takikawa, T., et al. (2021). Neural geometric level of detail: Real-time rendering with implicit 3D shapes. CVPR.","notes":"Hierarchical representation techniques - relevant to multi-resolution matrix approximation","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"liu2022neural","title":"Neural Sparse Voxel Fields","authors":"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2007.11571","keyPoints":"Sparse voxel representation with neural features; Efficient storage and rendering; Progressive training strategy","keyHypotheses":"Sparse voxel grids can efficiently represent complex scenes; Progressive training improves convergence","strengths":"Memory efficient; Fast rendering; Progressive optimization","weaknesses":"Voxelization artifacts; Limited to voxel resolution","citation":"Liu, L., et al. (2020). Neural sparse voxel fields. NeurIPS.","notes":"Sparse representations for efficiency - relevant to sparse matrix reconstruction approaches","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"niemeyer2022regnerf","title":"RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs","authors":"Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, Noha Radwan","journal":"CVPR","year":"2022","doi":"","url":"https://arxiv.org/abs/2112.00724","keyPoints":"Regularization for sparse view synthesis; Depth smoothness and normal consistency; Improved generalization with few views","keyHypotheses":"Geometric regularization improves sparse reconstruction; Smoothness priors prevent overfitting","strengths":"Better sparse view performance; Strong regularization framework; Improved generalization","weaknesses":"Additional regularization complexity; Hyperparameter sensitivity","citation":"Niemeyer, M., et al. (2022). RegNeRF: Regularizing neural radiance fields for view synthesis from sparse inputs. CVPR.","notes":"Regularization techniques for sparse reconstruction - directly applicable to sparse matrix completion","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"garbin2021fastnerf","title":"FastNeRF: High-Fidelity Neural Rendering at 200FPS","authors":"Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin","journal":"ICCV","year":"2021","doi":"","url":"https://arxiv.org/abs/2103.10380","keyPoints":"Factorized neural representation; Position-dependent and view-dependent components; Real-time rendering via caching","keyHypotheses":"Factorization enables real-time rendering; Caching strategies can accelerate inference","strengths":"Real-time performance; High quality rendering; Practical deployment","weaknesses":"Memory requirements for caching; Limited to specific scene types","citation":"Garbin, S. J., et al. (2021). FastNeRF: High-fidelity neural rendering at 200FPS. ICCV.","notes":"Real-time inference optimization - relevant to efficient matrix reconstruction deployment","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"riegler2021stable","title":"Stable View Synthesis","authors":"Gernot Riegler, Vladlen Koltun","journal":"CVPR","year":"2021,"doi":"","url":"https://arxiv.org/abs/2011.07233","keyPoints":"Stability analysis for view synthesis; Geometric consistency constraints; Robust training procedures","keyHypotheses":"Geometric constraints improve training stability; Consistency losses prevent mode collapse","strengths":"Training stability; Geometric consistency; Robust optimization","weaknesses":"Additional constraint complexity; Computational overhead","citation":"Riegler, G., & Koltun, V. (2021). Stable view synthesis. CVPR.","notes":"Training stability techniques - relevant to stable matrix reconstruction optimization","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"sun2022direct","title":"Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction","authors":"Cheng Sun, Min Sun, Hwann-Tzong Chen","journal":"CVPR","year":"2022","doi":"","url":"https://arxiv.org/abs/2111.11215","keyPoints":"Direct optimization of voxel grids; Super-fast convergence in minutes; No neural network required","keyHypotheses":"Direct grid optimization can match neural quality; Explicit optimization is more efficient than neural training","strengths":"Extremely fast training; Simple optimization; No neural components","weaknesses":"Memory scaling; Limited expressiveness compared to neural methods","citation":"Sun, C., Sun, M., & Chen, H. T. (2022). Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. CVPR.","notes":"Direct grid optimization approach - highly relevant to explicit vs implicit matrix reconstruction comparison","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"koltchinskii2011nuclear","title":"Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion","authors":"Vladimir Koltchinskii, Karim Lounici, Alexandre B. Tsybakov","journal":"Annals of Statistics","year":"2011","doi":"10.1214/10-AOS860","url":"https://projecteuclid.org/journals/annals-of-statistics/volume-39/issue-5/Nuclear-norm-penalization-and-optimal-rates-for-noisy-low-rank-matrix/10.1214/10-AOS860.full","keyPoints":"Optimal rates for noisy matrix completion; Nuclear norm penalization theory; Minimax lower bounds for matrix completion","keyHypotheses":"Nuclear norm penalization achieves optimal rates; Noisy observations require different analysis than noiseless case","strengths":"Rigorous theoretical analysis; Optimal rate characterization; Handles noisy observations","weaknesses":"Assumes low-rank structure; Restrictive sampling assumptions","citation":"Koltchinskii, V., Lounici, K., & Tsybakov, A. B. (2011). Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. Annals of Statistics, 39(5), 2302-2329.","notes":"Theoretical foundation for matrix completion rates - provides baseline for comparing INR-based approaches","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"jain2013low","title":"Low-rank matrix completion using alternating minimization","authors":"Prateek Jain, Praneeth Netrapalli, Sujay Sanghavi","journal":"STOC","year":"2013","doi":"10.1145/2488608.2488693","url":"https://arxiv.org/abs/1212.0467","keyPoints":"Alternating minimization for matrix completion; Non-convex optimization with global guarantees; Improved sample complexity bounds","keyHypotheses":"Non-convex alternating minimization can achieve global optimum; Sample complexity can be improved over convex methods","strengths":"Non-convex optimization theory; Better sample complexity; Practical algorithm","weaknesses":"Initialization sensitivity; Requires incoherence conditions","citation":"Jain, P., Netrapalli, P., & Sanghavi, S. (2013). Low-rank matrix completion using alternating minimization. STOC.","notes":"Non-convex optimization approach for matrix completion - relevant to neural optimization strategies","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"kolda2009tensor","title":"Tensor Decompositions and Applications","authors":"Tamara G. Kolda, Brett W. Bader","journal":"SIAM Review","year":"2009","doi":"10.1137/07070111X","url":"https://epubs.siam.org/doi/10.1137/07070111X","keyPoints":"Comprehensive survey of tensor decomposition methods; CANDECOMP/PARAFAC and Tucker decomposition; Applications across domains","keyHypotheses":"Tensor decomposition provides natural multiway data analysis; Different decompositions suitable for different applications","strengths":"Comprehensive survey; Mathematical rigor; Wide applicability","weaknesses":"Computational complexity; Local minima issues","citation":"Kolda, T. G., & Bader, B. W. (2009). Tensor decompositions and applications. SIAM Review, 51(3), 455-500.","notes":"Foundational tensor decomposition survey - basis for understanding TensoRF and K-Planes factorizations","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"sidiropoulos2017tensor","title":"Tensor Decomposition for Signal Processing and Machine Learning","authors":"Nicholas D. Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E. Papalexakis, Christos Faloutsos","journal":"IEEE Transactions on Signal Processing","year":"2017","doi":"10.1109/TSP.2017.2690524","url":"https://ieeexplore.ieee.org/document/7891546","keyPoints":"Modern tensor methods for ML; Tensor completion and compressed sensing; Deep learning connections","keyHypotheses":"Tensor methods can enhance machine learning; Deep networks have tensor structure; Completion algorithms generalize to tensors","strengths":"Modern perspective; ML connections; Comprehensive coverage","weaknesses":"Computational scaling; Implementation complexity","citation":"Sidiropoulos, N. D., et al. (2017). Tensor decomposition for signal processing and machine learning. IEEE Transactions on Signal Processing, 65(13), 3551-3582.","notes":"Modern tensor methods survey - connects classical decomposition to neural approaches","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"rauhut2017low","title":"Low rank tensor recovery via iterative hard thresholding","authors":"Holger Rauhut, Željka Stojanac","journal":"Linear Algebra and its Applications","year":"2017","doi":"10.1016/j.laa.2016.09.029","url":"https://arxiv.org/abs/1602.05217","keyPoints":"Iterative hard thresholding for tensor recovery; Non-convex optimization with guarantees; Sparse tensor completion","keyHypotheses":"Hard thresholding can recover low-rank tensors; Non-convex methods achieve better sample complexity","strengths":"Provable guarantees; Better sample complexity; Handles sparsity","weaknesses":"Parameter tuning; Initialization dependence","citation":"Rauhut, H., & Stojanac, Ž. (2017). Low rank tensor recovery via iterative hard thresholding. Linear Algebra and its Applications, 523, 220-262.","notes":"Non-convex tensor completion - relevant to understanding optimization landscapes in neural tensor methods","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"ongie2017tensor","title":"Tensor Methods for Nonlinear Matrix Completion","authors":"Gregory Ongie, Rebecca Willett, Robert D. Nowak, Laura Balzano","journal":"SIAM Journal on Mathematics of Data Science","year":"2019","doi":"10.1137/18M1200342","url":"https://arxiv.org/abs/1804.10266","keyPoints":"Nonlinear matrix completion using tensor methods; Lifting to higher dimensions; Polynomial matrix completion","keyHypotheses":"Tensor lifting enables nonlinear matrix completion; Higher-dimensional representations capture nonlinearity","strengths":"Handles nonlinear relationships; Theoretical guarantees; Novel lifting approach","weaknesses":"Increased dimensionality; Computational complexity","citation":"Ongie, G., Willett, R., Nowak, R. D., & Balzano, L. (2019). Tensor methods for nonlinear matrix completion. SIAM Journal on Mathematics of Data Science, 1(2), 253-285.","notes":"Nonlinear matrix completion - bridges classical methods with neural approaches that naturally handle nonlinearity","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"chen2019neural","title":"Neural Ordinary Differential Equations for Data-Driven Sparse Sensor Placement","authors":"Krithika Manohar, Eurika Kaiser, Steven L. Brunton, J. Nathan Kutz","journal":"arXiv","year":"2019","doi":"","url":"https://arxiv.org/abs/1906.00741","keyPoints":"Neural ODEs for sparse reconstruction; Data-driven sensor placement; Continuous-time neural dynamics","keyHypotheses":"Neural ODEs can optimize sensor placement; Continuous dynamics improve sparse reconstruction","strengths":"Novel application of Neural ODEs; Principled sensor placement; Continuous optimization","weaknesses":"ODE solver complexity; Limited to specific domains","citation":"Manohar, K., Kaiser, E., Brunton, S. L., & Kutz, J. N. (2019). Neural ordinary differential equations for data-driven sparse sensor placement. arXiv.","notes":"Neural ODEs for sparse problems - provides continuous perspective on discrete matrix completion","addedDate":"2025-09-20T00:24:00.000Z"}
{"id":"davenport2016overview","title":"An Overview of Low-Rank Matrix Recovery From Incomplete Observations","authors":"Mark A. Davenport, Yaniv Plan, Ewout van den Berg, Mary Wootters","journal":"IEEE Journal of Selected Topics in Signal Processing","year":"2016","doi":"10.1109/JSTSP.2016.2539100","url":"https://ieeexplore.ieee.org/document/7439790","keyPoints":"Comprehensive survey of matrix recovery; Theoretical guarantees and algorithms; Connections to compressed sensing","keyHypotheses":"Low-rank structure enables recovery from incomplete data; Theoretical limits are well-characterized","strengths":"Comprehensive survey; Theoretical depth; Algorithm comparison","weaknesses":"Limited to traditional methods; Pre-deep learning perspective","citation":"Davenport, M. A., Plan, Y., van den Berg, E., & Wootters, M. (2016). An overview of low-rank matrix recovery from incomplete observations. IEEE Journal of Selected Topics in Signal Processing, 10(4), 608-622.","notes":"Comprehensive traditional matrix completion survey - provides thorough baseline for comparing with INR approaches","addedDate":"2025-09-20T00:24:00.000Z"}
