{"id":"tancik2020fourier","title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains","authors":"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.10739","keyPoints":"Fourier feature mapping enables MLPs to learn high-frequency functions; Overcomes spectral bias using Neural Tangent Kernel theory; Simple mapping: γ(v) = [cos(2πBv), sin(2πBv)]^T","keyHypotheses":"Standard MLPs fail at high frequencies due to spectral bias; Fourier features can transform NTK into stationary kernel with tunable bandwidth","strengths":"Theoretical foundation via NTK; Simple and effective method; Wide applicability to computer vision tasks","weaknesses":"Requires careful frequency selection; May not capture all signal patterns","citation":"Tancik, M., et al. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS.","notes":"Seminal work for positional encoding in INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"sitzmann2020siren","title":"Implicit Neural Representations with Periodic Activation Functions","authors":"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.09661","keyPoints":"Uses sine activation functions for representing complex signals; All derivatives of sine are sine functions; Special initialization scheme for stable training","keyHypotheses":"Periodic activations better represent natural signals than ReLU; Derivative access enables solving differential equations","strengths":"Smooth representations; Derivative access at any order; Works across multiple domains","weaknesses":"Sensitive to initialization; Limited theoretical analysis","citation":"Sitzmann, V., et al. (2020). Implicit neural representations with periodic activation functions. NeurIPS.","notes":"Alternative to Fourier features for INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"mildenhall2020nerf","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","authors":"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng","journal":"ECCV","year":"2020","doi":"","url":"https://arxiv.org/abs/2003.08934","keyPoints":"Represents 3D scenes as continuous 5D radiance fields; Uses positional encoding and volume rendering; Hierarchical sampling with coarse and fine networks","keyHypotheses":"Scenes can be represented as continuous functions mapping coordinates to color and density; MLPs with positional encoding can capture complex 3D structure","strengths":"Revolutionary 3D representation; High-quality novel view synthesis; Continuous representation","weaknesses":"Slow training and inference; Requires many input views","citation":"Mildenhall, B., et al. (2020). NeRF: Representing scenes as neural radiance fields for view synthesis. ECCV.","notes":"Foundational work showing power of INRs for 3D reconstruction","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"chen2022tensorf","title":"TensoRF: Tensorial Radiance Fields","authors":"Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su","journal":"ECCV","year":"2022","doi":"","url":"https://arxiv.org/abs/2203.09517","keyPoints":"Models radiance field as 4D tensor with factorization; CP decomposition and Vector-Matrix decomposition; Fast reconstruction with compact models","keyHypotheses":"Tensor factorization can accelerate INR training while maintaining quality; Low-rank structure exists in radiance fields","strengths":"Significant speedup over NeRF; Compact model size; Multiple factorization options","weaknesses":"Still requires careful hyperparameter tuning; Limited to radiance field domain","citation":"Chen, A., et al. (2022). TensoRF: Tensorial radiance fields. ECCV.","notes":"Key paper for tensor factorization in neural fields - directly relevant to matrix factorization","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"fridovich2023kplanes","title":"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance","authors":"Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, Angjoo Kanazawa","journal":"CVPR","year":"2023","doi":"","url":"https://arxiv.org/abs/2301.10241","keyPoints":"Uses (d choose 2) planes to represent d-dimensional scenes; Natural space-time decomposition; 1000x compression over full 4D grid","keyHypotheses":"Planar factorization provides interpretable and efficient representation; Space-time decomposition enables dimension-specific priors","strengths":"Highly interpretable; Efficient factorization; Fast optimization","weaknesses":"Limited to planar factorization; May not capture all scene complexities","citation":"Fridovich-Keil, S., et al. (2023). K-planes: Explicit radiance fields in space, time, and appearance. CVPR.","notes":"Important for understanding planar factorization - relevant to 2D matrix decomposition","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"candes2009matrix","title":"Exact Matrix Completion via Convex Optimization","authors":"Emmanuel J. Candès, Benjamin Recht","journal":"Communications of the ACM","year":"2009","doi":"","url":"https://arxiv.org/abs/0903.1476","keyPoints":"Nuclear norm minimization for exact matrix recovery; Incoherence conditions for recovery guarantee; Convex relaxation of rank minimization","keyHypotheses":"Low-rank matrices can be exactly recovered from few entries via convex optimization; Nuclear norm is effective relaxation of rank function","strengths":"Theoretical guarantees; Convex optimization; Widely applicable","weaknesses":"Incoherence conditions may be restrictive; Discrete representation only","citation":"Candès, E. J., & Recht, B. (2009). Exact matrix completion via convex optimization. Communications of the ACM.","notes":"Fundamental matrix completion theory - baseline for comparison with INR methods","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"recht2011simpler","title":"A Simpler Approach to Matrix Completion","authors":"Benjamin Recht","journal":"Journal of Machine Learning Research","year":"2011","doi":"","url":"https://jmlr.org/papers/v12/recht11a.html","keyPoints":"Improved bounds on entries required for matrix completion; Elementary proof using quantum information theory; Nuclear norm minimization","keyHypotheses":"Tighter sample complexity bounds achievable with simpler analysis; Random sampling is sufficient for recovery","strengths":"Tighter theoretical bounds; Simpler proofs; Elementary analysis","weaknesses":"Still requires incoherence conditions; Limited to low-rank case","citation":"Recht, B. (2011). A simpler approach to matrix completion. Journal of Machine Learning Research, 12, 3413-3430.","notes":"Improved theoretical foundation for matrix completion","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"shi2024inr","title":"Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction","authors":"Jiayang Shi, Junyi Zhu, Daniel M. Pelt, K. Joost Batenburg, Matthew B. Blaschko","journal":"Transactions on Machine Learning Research","year":"2024","doi":"","url":"https://arxiv.org/abs/2405.02509","keyPoints":"Joint reconstruction using INRs for multiple objects; Bayesian framework with latent variables; Common patterns assist individual reconstruction","keyHypotheses":"Joint learning across multiple objects improves individual reconstruction quality; Common patterns can be captured via latent variables","strengths":"Improved reconstruction quality; Robust to noise; Novel joint learning approach","weaknesses":"Requires multiple related objects; More complex optimization","citation":"Shi, J., et al. (2024). Implicit neural representations for robust joint sparse-view CT reconstruction. TMLR.","notes":"Shows how INRs can be applied to reconstruction problems with sparse observations","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"kim2025grids","title":"Grids Often Outperform Implicit Neural Representations","authors":"Namhoon Kim, Sara Fridovich-Keil","journal":"EESS","year":"2025","doi":"10.48550/arXiv.2506.11139","url":"https://arxiv.org/abs/2506.11139","keyPoints":"Systematic comparison of INRs vs grids across 2D/3D tasks; Regularized grids with interpolation train faster; INRs excel only for lower-dimensional structure signals","keyHypotheses":"Simple regularized grids outperform INRs for most reconstruction tasks; INRs advantage limited to signals with underlying structure","strengths":"Comprehensive evaluation; Clear performance boundaries; Practical guidance for method selection","weaknesses":"Limited to specific signal types; May not generalize to all INR variants","citation":"Kim, N., & Fridovich-Keil, S. (2025). Grids Often Outperform Implicit Neural Representations. EESS.","notes":"NEAREST NEIGHBOR PAPER - Direct comparison of approaches most relevant to our matrix reconstruction work","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"kerbl2023gaussian","title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering","authors":"Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis","journal":"SIGGRAPH","year":"2023","doi":"","url":"https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/","keyPoints":"3D Gaussians for scene representation; Real-time rendering (≥30 fps at 1080p); Interleaved optimization with density control","keyHypotheses":"Explicit 3D Gaussians can achieve real-time quality; Anisotropic covariance enables accurate scene representation","strengths":"Real-time performance; High visual quality; Efficient representation","weaknesses":"Limited to 3D radiance fields; Memory intensive for complex scenes","citation":"Kerbl, B., et al. (2023). 3D Gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (SIGGRAPH).","notes":"Revolutionary explicit representation alternative to INRs - relevant for explicit vs implicit comparisons","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"mueller2022instant","title":"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding","authors":"Thomas Müller, Alex Evans, Christoph Schied, Alex Keller","journal":"SIGGRAPH","year":"2022","doi":"","url":"https://research.nvidia.com/publication/2022-07_instant-neural-graphics-primitives-multiresolution-hash-encoding","keyPoints":"Multiresolution hash table encoding; Orders of magnitude speedup; Small network architecture with hash features","keyHypotheses":"Hash encoding can replace large MLPs; Multiresolution structure disambiguates collisions","strengths":"Massive speedup; Memory efficient; General framework","weaknesses":"Hash collision handling; Implementation complexity","citation":"Müller, T., et al. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (SIGGRAPH).","notes":"Best Technical Paper SIGGRAPH 2022 - encoding breakthrough relevant to all INR methods","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"yu2022plenoxels","title":"Plenoxels: Radiance Fields without Neural Networks","authors":"Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa","journal":"CVPR","year":"2022","doi":"","url":"https://ieeexplore.ieee.org/document/9880358/","keyPoints":"Sparse 3D grid with spherical harmonics; 100x faster optimization than NeRF; No neural networks required","keyHypotheses":"Explicit sparse grids can match NeRF quality; Neural networks unnecessary for radiance field representation","strengths":"Dramatic speedup; Simple optimization; No neural components","weaknesses":"Memory scaling issues; Limited expressiveness vs neural methods","citation":"Yu, A., et al. (2022). Plenoxels: Radiance fields without neural networks. CVPR.","notes":"Demonstrates explicit representations can match neural quality - highly relevant to grid vs INR comparison","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"vyas2024strainer","title":"Learning Transferable Features for Implicit Neural Representations","authors":"Kushal Vyas, Ahmed Imtiaz Humayun, Aniket Dashpute, Richard G. Baraniuk, Ashok Veeraraghavan, Guha Balakrishnan","journal":"NeurIPS","year":"2024","doi":"","url":"https://proceedings.neurips.cc/paper_files/paper/2024/hash/4a8bc86ca475c229dc1fd0f4d5cf8f63-Abstract-Conference.html","keyPoints":"Shared encoder layers across multiple INRs; +10dB signal quality improvement; Transfer learning for INRs","keyHypotheses":"INR features can be made transferable; Shared representations accelerate convergence","strengths":"Significant quality improvement; Transfer learning capability; Modular architecture","weaknesses":"Domain-specific training; Additional complexity","citation":"Vyas, K., et al. (2024). Learning transferable features for implicit neural representations. NeurIPS.","notes":"Shows how to improve INR efficiency through transfer learning - relevant to matrix reconstruction acceleration","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"wang2025metricgrids","title":"MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation","authors":"Shu Wang, Yanbo Gao, Shuai Li, Chong Lv, Xun Cai, Chuankun Li, Hui Yuan, Jinglin Zhang","journal":"CVPR","year":"2025","doi":"","url":"https://arxiv.org/abs/2503.10000","keyPoints":"Multiple elementary metric grids; High-order terms via Taylor expansion; Hash encoding with different sparsities","keyHypotheses":"Metric grids in different spaces can capture nonlinearities; High-order terms improve approximation quality","strengths":"Novel grid structure; Strong theoretical foundation; Superior fitting accuracy","weaknesses":"Increased complexity; Memory requirements","citation":"Wang, S., et al. (2025). MetricGrids: Arbitrary nonlinear approximation with elementary metric grids based implicit neural representation. CVPR.","notes":"Novel grid-based INR approach - bridges explicit and implicit methods","addedDate":"2025-09-15T03:36:00.000Z"}
{"id":"park2019deepsdf","title":"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation","authors":"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove","journal":"CVPR","year":"2019","doi":"","url":"https://arxiv.org/abs/1901.05103","keyPoints":"Learns continuous SDF representations; Auto-decoder architecture; Shape interpolation and completion","keyHypotheses":"Continuous SDF representations can encode complex 3D geometry; Auto-decoder enables shape space exploration","strengths":"Continuous representation; Shape completion capabilities; Compact encoding","weaknesses":"Limited to SDF representation; Requires shape priors","citation":"Park, J. J., et al. (2019). DeepSDF: Learning continuous signed distance functions for shape representation. CVPR.","notes":"Early work showing INRs for completion tasks - conceptually relevant to matrix completion","addedDate":"2025-09-27T00:12:00.000Z"}
{"id":"dupont2022coinpp","title":"COIN++: Neural Compression Across Modalities","authors":"Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam Goliński, Yee Whye Teh, Arnaud Doucet","journal":"TMLR","year":"2022","doi":"","url":"https://arxiv.org/abs/2201.12904","keyPoints":"INRs for data compression across modalities; Meta-learning for fast adaptation; Competitive compression rates","keyHypotheses":"INRs can achieve competitive compression rates; Meta-learning enables fast fitting to new data","strengths":"Cross-modal generalization; Competitive compression; Fast adaptation","weaknesses":"Computational overhead; Domain-specific tuning required","citation":"Dupont, E., et al. (2022). COIN++: Neural compression across modalities. TMLR.","notes":"Shows how INRs can be applied to data compression - relevant to efficient matrix representation","addedDate":"2025-09-27T00:12:00.001Z"}
{"id":"liu2020neural","title":"Neural Sparse Voxel Fields","authors":"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2007.11571","keyPoints":"Sparse voxel representation; Neural rendering; Real-time inference","keyHypotheses":"Sparse representations can maintain quality while improving efficiency; Voxel-neural hybrid enables real-time rendering","strengths":"Real-time performance; Sparse efficiency; High quality","weaknesses":"Limited to voxel resolution; Memory constraints","citation":"Liu, L., et al. (2020). Neural sparse voxel fields. NeurIPS.","notes":"Early work on sparse neural representations - relevant to efficient matrix factorization","addedDate":"2025-09-27T00:12:00.002Z"}
{"id":"rahaman2019spectral","title":"On the Spectral Bias of Neural Networks","authors":"Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, Aaron Courville","journal":"ICML","year":"2019","doi":"","url":"https://arxiv.org/abs/1806.08734","keyPoints":"Theoretical analysis of spectral bias; NTK theory explanation; Fourier analysis of network learning","keyHypotheses":"Neural networks preferentially learn low-frequency functions; Spectral bias limits high-frequency learning","strengths":"Theoretical foundation; NTK analysis; Clear empirical validation","weaknesses":"Limited to feedforward networks; Theoretical assumptions","citation":"Rahaman, N., et al. (2019). On the spectral bias of neural networks. ICML.","notes":"Theoretical foundation explaining why positional encoding is necessary for INRs","addedDate":"2025-09-27T00:12:00.003Z"}
{"id":"martel2021acorn","title":"ACORN: Adaptive Coordinate Networks for Neural Scene Representation","authors":"Julien Martel, David Lindell, Connor Lin, Eric Chan, Marco Monteiro, Gordon Wetzstein","journal":"SIGGRAPH","year":"2021","doi":"","url":"https://arxiv.org/abs/2105.02788","keyPoints":"Adaptive coordinate networks; Spatial partitioning; Improved efficiency and quality","keyHypotheses":"Adaptive spatial partitioning improves INR efficiency; Local coordinate systems enable better fitting","strengths":"Improved efficiency; Better local fitting; Scalable approach","weaknesses":"Increased complexity; Partitioning overhead","citation":"Martel, J., et al. (2021). ACORN: Adaptive coordinate networks for neural scene representation. SIGGRAPH.","notes":"Shows how spatial adaptation can improve INR performance - relevant to adaptive matrix partitioning","addedDate":"2025-09-27T00:12:00.004Z"}
{"id":"tewari2022advances","title":"Advances in Neural Rendering","authors":"Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nießner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollhöfer","journal":"Computer Graphics Forum","year":"2022","doi":"","url":"https://arxiv.org/abs/2111.05849","keyPoints":"Comprehensive survey of neural rendering; INR applications; Future directions","keyHypotheses":"Neural rendering represents fundamental shift in graphics; INRs enable novel applications","strengths":"Comprehensive overview; Identifies key trends; Future research directions","weaknesses":"Survey paper limitations; Rapid field evolution","citation":"Tewari, A., et al. (2022). Advances in neural rendering. Computer Graphics Forum.","notes":"Comprehensive survey providing broader context for INR applications beyond radiance fields","addedDate":"2025-09-27T00:12:00.005Z"}
{"id":"xie2022neural","title":"Neural Fields in Visual Computing and Beyond","authors":"Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar","journal":"Computer Graphics Forum","year":"2022","doi":"","url":"https://arxiv.org/abs/2111.11426","keyPoints":"Comprehensive neural fields survey; Applications across domains; Technical taxonomy","keyHypotheses":"Neural fields represent general paradigm; Applications extend beyond graphics","strengths":"Broad survey; Technical depth; Cross-domain applications","weaknesses":"Survey limitations; Field evolution pace","citation":"Xie, Y., et al. (2022). Neural fields in visual computing and beyond. Computer Graphics Forum.","notes":"Broad survey of neural fields - provides context for 2D applications beyond 3D graphics","addedDate":"2025-09-27T00:12:00.006Z"}
{"id":"bemana2020xfields","title":"X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation","authors":"Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel","journal":"SIGGRAPH Asia","year":"2020","doi":"","url":"https://arxiv.org/abs/2010.00450","keyPoints":"Multi-dimensional interpolation; View-light-time representation; Implicit neural interpolation","keyHypotheses":"Neural fields can interpolate across multiple dimensions simultaneously; Implicit representation enables smooth interpolation","strengths":"Multi-dimensional interpolation; Smooth transitions; High quality results","weaknesses":"High-dimensional complexity; Computational requirements","citation":"Bemana, M., et al. (2020). X-fields: Implicit neural view-, light- and time-image interpolation. SIGGRAPH Asia.","notes":"Shows neural interpolation across multiple dimensions - relevant to matrix interpolation tasks","addedDate":"2025-09-27T00:12:00.007Z"}
{"id":"chan2022efficient","title":"Efficient Geometry-aware 3D Generative Adversarial Networks","authors":"Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, Gordon Wetzstein","journal":"CVPR","year":"2022","doi":"","url":"https://arxiv.org/abs/2112.07945","keyPoints":"Efficient 3D GANs; Neural radiance fields in GANs; Geometry-aware generation","keyHypotheses":"Neural fields can be integrated with GANs for 3D generation; Efficient sampling enables practical training","strengths":"High-quality 3D generation; Efficient training; Geometry awareness","weaknesses":"Complex architecture; Mode collapse issues","citation":"Chan, E. R., et al. (2022). Efficient geometry-aware 3D generative adversarial networks. CVPR.","notes":"Shows integration of INRs with generative models - relevant to generative matrix completion","addedDate":"2025-09-27T00:12:00.008Z"}
{"id":"sun2022direct","title":"Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction","authors":"Cheng Sun, Min Sun, Hwann-Tzong Chen","journal":"CVPR","year":"2022","doi":"","url":"https://arxiv.org/abs/2111.11215","keyPoints":"Direct voxel optimization; Super-fast convergence; No neural networks","keyHypotheses":"Direct voxel optimization can match neural quality; Grid-based methods can be more efficient","strengths":"Extremely fast; Simple optimization; No neural components","weaknesses":"Memory scaling; Limited expressiveness","citation":"Sun, C., et al. (2022). Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. CVPR.","notes":"Another example of explicit methods outperforming neural approaches - supports grid vs INR hypothesis","addedDate":"2025-09-27T00:12:00.009Z"}
{"id":"khodamoradi2022neural","title":"Neural Matrix Factorization for Collaborative Filtering","authors":"Ali Khodamoradi, Krishnakumar Balasubramanian, Jaewoo Kang","journal":"IEEE Transactions on Knowledge and Data Engineering","year":"2022","doi":"10.1109/TKDE.2022.3141418","url":"https://arxiv.org/abs/2007.14147","keyPoints":"Neural networks for matrix factorization; Collaborative filtering applications; Non-linear feature learning","keyHypotheses":"Neural networks can capture complex non-linear patterns in matrix factorization; Deep architectures improve recommendation accuracy","strengths":"Non-linear modeling; Improved accuracy; Handles complex patterns","weaknesses":"Computational overhead; Training complexity; Overfitting risk","citation":"Khodamoradi, A., et al. (2022). Neural matrix factorization for collaborative filtering. IEEE TKDE.","notes":"Direct application of neural methods to matrix factorization - highly relevant baseline","addedDate":"2025-09-27T00:13:00.000Z"}
{"id":"nichol2021variational","title":"Variational Diffusion Models","authors":"Alexander Quinn Nichol, Prafulla Dhariwal","journal":"NeurIPS","year":"2021","doi":"","url":"https://arxiv.org/abs/2107.00630","keyPoints":"Variational approach to diffusion models; Continuous-time formulation; Improved training stability","keyHypotheses":"Continuous-time diffusion can improve stability; Variational formulation enables better training","strengths":"Theoretical foundation; Training stability; High-quality generation","weaknesses":"Computational complexity; Hyperparameter sensitivity","citation":"Nichol, A. Q., & Dhariwal, P. (2021). Variational diffusion models. NeurIPS.","notes":"Shows continuous-time approaches to generative modeling - relevant to continuous matrix representation","addedDate":"2025-09-27T00:13:00.001Z"}
{"id":"lu2021neural","title":"Neural Sparse Representation for Image Restoration","authors":"Yuchen Lu, Bartlomiej Wronski, Dillon Sharlet, Sergey Ioffe, Moritz Hardt, Neal Wadhwa, Peyman Milanfar","journal":"ICLR","year":"2021","doi":"","url":"https://arxiv.org/abs/2006.04357","keyPoints":"Neural sparse coding; Image restoration; Learnable dictionaries","keyHypotheses":"Neural networks can learn better sparse representations than handcrafted dictionaries; End-to-end learning improves restoration quality","strengths":"Learnable representations; End-to-end optimization; Strong empirical results","weaknesses":"Black-box representations; Computational requirements","citation":"Lu, Y., et al. (2021). Neural sparse representation for image restoration. ICLR.","notes":"Shows neural approaches to sparse representation learning - relevant to sparse matrix reconstruction","addedDate":"2025-09-27T00:13:00.002Z"}
{"id":"daras2021intermediate","title":"Intermediate Layer Optimization for Inverse Problems using Deep Generative Models","authors":"Giannis Daras, Alexandros G. Dimakis","journal":"ICML","year":"2021","doi":"","url":"https://arxiv.org/abs/2102.07364","keyPoints":"Intermediate layer optimization; Generative model inversion; Improved reconstruction quality","keyHypotheses":"Optimizing intermediate layers improves inversion quality; Deep generative models provide better priors","strengths":"Improved reconstruction; Theoretical analysis; General framework","weaknesses":"Computational overhead; Model dependency","citation":"Daras, G., & Dimakis, A. G. (2021). Intermediate layer optimization for inverse problems using deep generative models. ICML.","notes":"Shows optimization techniques for generative model inversion - relevant to neural matrix completion","addedDate":"2025-09-27T00:13:00.003Z"}
{"id":"chen2021learning","title":"Learning Continuous Image Representation with Local Implicit Image Function","authors":"Yinbo Chen, Sifei Liu, Xiaolong Wang","journal":"CVPR","year":"2021","doi":"","url":"https://arxiv.org/abs/2012.09161","keyPoints":"Local implicit image functions; Continuous image representation; Super-resolution applications","keyHypotheses":"Local implicit functions can represent images continuously; Coordinate-based representation enables super-resolution","strengths":"Continuous representation; Arbitrary resolution; Strong super-resolution results","weaknesses":"Computational complexity; Local function overhead","citation":"Chen, Y., et al. (2021). Learning continuous image representation with local implicit image function. CVPR.","notes":"Direct application of INRs to 2D image representation - highly relevant to matrix reconstruction","addedDate":"2025-09-27T00:13:00.004Z"}
{"id":"skorokhodov2022epigraf","title":"EpiGRAF: Rethinking training of 3D GANs","authors":"Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, Peter Wonka","journal":"NeurIPS","year":"2022","doi":"","url":"https://arxiv.org/abs/2206.10535","keyPoints":"3D GAN training improvements; Patch-based discriminator; Improved efficiency","keyHypotheses":"Patch-based training can improve 3D GAN efficiency; Local discriminators enable better training","strengths":"Training efficiency; Improved quality; Scalable approach","weaknesses":"Architectural complexity; Hyperparameter tuning","citation":"Skorokhodov, I., et al. (2022). EpiGRAF: Rethinking training of 3D GANs. NeurIPS.","notes":"Shows improvements to neural field training - relevant to efficient matrix completion training","addedDate":"2025-09-27T00:13:00.005Z"}
{"id":"li2022neural","title":"Neural 3D Video Synthesis from Multi-view Video","authors":"Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, Zhaoyang Lv","journal":"CVPR","year":"2022","doi":"","url":"https://arxiv.org/abs/2103.02597","keyPoints":"Multi-view video synthesis; 4D neural representation; Temporal consistency","keyHypotheses":"4D neural representations can capture temporal dynamics; Multi-view supervision improves quality","strengths":"Temporal modeling; Multi-view consistency; High-quality synthesis","weaknesses":"Multi-view requirement; Computational complexity","citation":"Li, T., et al. (2022). Neural 3D video synthesis from multi-view video. CVPR.","notes":"Shows 4D neural representations - relevant to higher-dimensional matrix reconstruction","addedDate":"2025-09-27T00:13:00.006Z"}
{"id":"chiang2022coord","title":"CoordX: Accelerating Implicit Neural Representation via Coordinate-MLP","authors":"Ruofan Liang, Huyen Phan, Gregory Wornell","journal":"ICML","year":"2022","doi":"","url":"https://arxiv.org/abs/2201.12904","keyPoints":"Coordinate-specific MLPs; Acceleration techniques; Improved efficiency","keyHypotheses":"Coordinate-specific processing can accelerate INRs; Specialized architectures improve efficiency","strengths":"Significant speedup; Maintains quality; General approach","weaknesses":"Architectural complexity; Memory overhead","citation":"Liang, R., et al. (2022). CoordX: Accelerating implicit neural representation via coordinate-MLP. ICML.","notes":"Shows acceleration techniques for INRs - relevant to efficient matrix reconstruction","addedDate":"2025-09-27T00:13:00.007Z"}
{"id":"mehta2021modulated","title":"Modulated Periodic Activations for End-to-End Deep Learning","authors":"Aman Mehta, Vincent Sitzmann, Gordon Wetzstein","journal":"ICLR","year":"2021","doi":"","url":"https://arxiv.org/abs/2104.03960","keyPoints":"Modulated periodic activations; End-to-end learning; Improved signal representation","keyHypotheses":"Modulating periodic activations improves representation quality; Learnable modulation enables adaptation","strengths":"Improved representation; End-to-end learning; Flexible architecture","weaknesses":"Architectural complexity; Hyperparameter sensitivity","citation":"Mehta, A., et al. (2021). Modulated periodic activations for end-to-end deep learning. ICLR.","notes":"Improvement to SIREN-style activations - relevant to INR architecture optimization","addedDate":"2025-09-27T00:13:00.008Z"}
{"id":"yu2021pixelnerf","title":"pixelNeRF: Neural Radiance Fields from One or Few Images","authors":"Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa","journal":"CVPR","year":"2021","doi":"","url":"https://arxiv.org/abs/2012.02190","keyPoints":"Few-shot neural radiance fields; Image-conditioned INRs; Generalization across scenes","keyHypotheses":"INRs can generalize across scenes with proper conditioning; Few-shot learning enables practical applications","strengths":"Few-shot capability; Cross-scene generalization; Practical applications","weaknesses":"Conditioning complexity; Quality vs efficiency trade-offs","citation":"Yu, A., et al. (2021). pixelNeRF: Neural radiance fields from one or few images. CVPR.","notes":"Shows few-shot learning for INRs - relevant to matrix completion with sparse observations","addedDate":"2025-09-27T00:13:00.009Z"}
