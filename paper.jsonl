{"id":"tancik2020fourier","title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains","authors":"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.10739","keyPoints":"Fourier feature mapping enables MLPs to learn high-frequency functions; Overcomes spectral bias using Neural Tangent Kernel theory; Simple mapping: γ(v) = [cos(2πBv), sin(2πBv)]^T","keyHypotheses":"Standard MLPs fail at high frequencies due to spectral bias; Fourier features can transform NTK into stationary kernel with tunable bandwidth","strengths":"Theoretical foundation via NTK; Simple and effective method; Wide applicability to computer vision tasks","weaknesses":"Requires careful frequency selection; May not capture all signal patterns","citation":"Tancik, M., et al. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS.","notes":"Seminal work for positional encoding in INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"sitzmann2020siren","title":"Implicit Neural Representations with Periodic Activation Functions","authors":"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein","journal":"NeurIPS","year":"2020","doi":"","url":"https://arxiv.org/abs/2006.09661","keyPoints":"Uses sine activation functions for representing complex signals; All derivatives of sine are sine functions; Special initialization scheme for stable training","keyHypotheses":"Periodic activations better represent natural signals than ReLU; Derivative access enables solving differential equations","strengths":"Smooth representations; Derivative access at any order; Works across multiple domains","weaknesses":"Sensitive to initialization; Limited theoretical analysis","citation":"Sitzmann, V., et al. (2020). Implicit neural representations with periodic activation functions. NeurIPS.","notes":"Alternative to Fourier features for INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"mildenhall2020nerf","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","authors":"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng","journal":"ECCV","year":"2020","doi":"","url":"https://arxiv.org/abs/2003.08934","keyPoints":"Represents 3D scenes as continuous 5D radiance fields; Uses positional encoding and volume rendering; Hierarchical sampling with coarse and fine networks","keyHypotheses":"Scenes can be represented as continuous functions mapping coordinates to color and density; MLPs with positional encoding can capture complex 3D structure","strengths":"Revolutionary 3D representation; High-quality novel view synthesis; Continuous representation","weaknesses":"Slow training and inference; Requires many input views","citation":"Mildenhall, B., et al. (2020). NeRF: Representing scenes as neural radiance fields for view synthesis. ECCV.","notes":"Foundational work showing power of INRs for 3D reconstruction","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"chen2022tensorf","title":"TensoRF: Tensorial Radiance Fields","authors":"Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su","journal":"ECCV","year":"2022","doi":"","url":"https://arxiv.org/abs/2203.09517","keyPoints":"Models radiance field as 4D tensor with factorization; CP decomposition and Vector-Matrix decomposition; Fast reconstruction with compact models","keyHypotheses":"Tensor factorization can accelerate INR training while maintaining quality; Low-rank structure exists in radiance fields","strengths":"Significant speedup over NeRF; Compact model size; Multiple factorization options","weaknesses":"Still requires careful hyperparameter tuning; Limited to radiance field domain","citation":"Chen, A., et al. (2022). TensoRF: Tensorial radiance fields. ECCV.","notes":"Key paper for tensor factorization in neural fields - directly relevant to matrix factorization","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"fridovich2023kplanes","title":"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance","authors":"Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, Angjoo Kanazawa","journal":"CVPR","year":"2023","doi":"","url":"https://arxiv.org/abs/2301.10241","keyPoints":"Uses (d choose 2) planes to represent d-dimensional scenes; Natural space-time decomposition; 1000x compression over full 4D grid","keyHypotheses":"Planar factorization provides interpretable and efficient representation; Space-time decomposition enables dimension-specific priors","strengths":"Highly interpretable; Efficient factorization; Fast optimization","weaknesses":"Limited to planar factorization; May not capture all scene complexities","citation":"Fridovich-Keil, S., et al. (2023). K-planes: Explicit radiance fields in space, time, and appearance. CVPR.","notes":"Important for understanding planar factorization - relevant to 2D matrix decomposition","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"candes2009matrix","title":"Exact Matrix Completion via Convex Optimization","authors":"Emmanuel J. Candès, Benjamin Recht","journal":"Communications of the ACM","year":"2009","doi":"","url":"https://arxiv.org/abs/0903.1476","keyPoints":"Nuclear norm minimization for exact matrix recovery; Incoherence conditions for recovery guarantee; Convex relaxation of rank minimization","keyHypotheses":"Low-rank matrices can be exactly recovered from few entries via convex optimization; Nuclear norm is effective relaxation of rank function","strengths":"Theoretical guarantees; Convex optimization; Widely applicable","weaknesses":"Incoherence conditions may be restrictive; Discrete representation only","citation":"Candès, E. J., & Recht, B. (2009). Exact matrix completion via convex optimization. Communications of the ACM.","notes":"Fundamental matrix completion theory - baseline for comparison with INR methods","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"recht2011simpler","title":"A Simpler Approach to Matrix Completion","authors":"Benjamin Recht","journal":"Journal of Machine Learning Research","year":"2011","doi":"","url":"https://jmlr.org/papers/v12/recht11a.html","keyPoints":"Improved bounds on entries required for matrix completion; Elementary proof using quantum information theory; Nuclear norm minimization","keyHypotheses":"Tighter sample complexity bounds achievable with simpler analysis; Random sampling is sufficient for recovery","strengths":"Tighter theoretical bounds; Simpler proofs; Elementary analysis","weaknesses":"Still requires incoherence conditions; Limited to low-rank case","citation":"Recht, B. (2011). A simpler approach to matrix completion. Journal of Machine Learning Research, 12, 3413-3430.","notes":"Improved theoretical foundation for matrix completion","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"shi2024inr","title":"Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction","authors":"Jiayang Shi, Junyi Zhu, Daniel M. Pelt, K. Joost Batenburg, Matthew B. Blaschko","journal":"Transactions on Machine Learning Research","year":"2024","doi":"","url":"https://arxiv.org/abs/2405.02509","keyPoints":"Joint reconstruction using INRs for multiple objects; Bayesian framework with latent variables; Common patterns assist individual reconstruction","keyHypotheses":"Joint learning across multiple objects improves individual reconstruction quality; Common patterns can be captured via latent variables","strengths":"Improved reconstruction quality; Robust to noise; Novel joint learning approach","weaknesses":"Requires multiple related objects; More complex optimization","citation":"Shi, J., et al. (2024). Implicit neural representations for robust joint sparse-view CT reconstruction. TMLR.","notes":"Shows how INRs can be applied to reconstruction problems with sparse observations","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"zhang2025lorein","title":"Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction","authors":"Haonan Zhang, Guoyan Lao, Yuyao Zhang, Hongjiang Wei","journal":"arXiv preprint","year":"2025","doi":"","url":"https://arxiv.org/abs/2506.09100","keyPoints":"Combines low-rank prior with INR continuity prior; LoREIN framework for MRI reconstruction; Zero-shot learning paradigm","keyHypotheses":"Dual priors (low-rank + continuity) enhance reconstruction fidelity; INR provides natural continuity prior","strengths":"Unsupervised learning; Dual prior integration; High-dimensional capability","weaknesses":"Complex framework; Limited evaluation domains","citation":"Zhang, H., et al. (2025). Low-rank augmented implicit neural representation for unsupervised high-dimensional quantitative MRI reconstruction. arXiv preprint.","notes":"Direct combination of low-rank methods with INRs - highly relevant to matrix reconstruction","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"li2025imputeinr","title":"ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data","authors":"Mengxuan Li, Ke Liu, Jialong Guo, Jiajun Bu, Hongwei Wang, Haishuai Wang","journal":"IJCAI","year":"2025","doi":"","url":"https://arxiv.org/abs/2505.10856","keyPoints":"Continuous time series representation via INRs; Not coupled to sampling frequency; Superior performance on high missing ratios","keyHypotheses":"INRs can model sparse temporal data better than discrete methods; Continuous functions enable fine-grained imputation","strengths":"Works well with sparse data; Continuous representation; Healthcare applications","weaknesses":"Limited to time series domain; Computational complexity","citation":"Li, M., et al. (2025). ImputeINR: Time series imputation via implicit neural representations for disease diagnosis with missing data. IJCAI.","notes":"Shows INR effectiveness for imputation tasks - similar to matrix completion","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"li2025mgir","title":"Mixed-granularity Implicit Representation for Continuous Hyperspectral Compressive Reconstruction","authors":"Jianan Li, Huan Chen, Wangcai Zhao, Rui Chen, Tingfa Xu","journal":"IEEE Transactions on Neural Networks and Learning Systems","year":"2025","doi":"","url":"https://arxiv.org/abs/2503.12783","keyPoints":"Mixed Granularity Implicit Representation framework; Hierarchical spectral-spatial implicit encoder; Reconstruction at arbitrary spatial-spectral resolutions","keyHypotheses":"Mixed-granularity representation enables flexible resolution reconstruction; Hierarchical encoding captures multi-scale features","strengths":"Arbitrary resolution reconstruction; Multi-scale features; State-of-the-art performance","weaknesses":"Complex architecture; Specific to hyperspectral domain","citation":"Li, J., et al. (2025). Mixed-granularity implicit representation for continuous hyperspectral compressive reconstruction. IEEE TNNLS.","notes":"Multi-resolution reconstruction techniques applicable to matrix problems","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"rao2025cristal","title":"Implicit Neural Representation-Based MRI Reconstruction Method with Sensitivity Map Constraints","authors":"Lixuan Rao, Xinlin Zhang, Yiman Huang, Tao Tan, Tong Tong","journal":"arXiv preprint","year":"2025","doi":"","url":"https://arxiv.org/abs/2506.06043","keyPoints":"Joint coil sensitivity and image estimation; INR-CRISTAL with sensitivity map regularization; Superior artifact removal","keyHypotheses":"Joint estimation improves reconstruction quality; Sensitivity map characteristics can be exploited for regularization","strengths":"No training data needed; Joint estimation framework; Robust performance","weaknesses":"MRI-specific constraints; Limited generalization","citation":"Rao, L., et al. (2025). Implicit neural representation-based MRI reconstruction method with sensitivity map constraints. arXiv preprint.","notes":"Shows how domain-specific constraints can be integrated with INRs","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"sivgin2024gaplanes","title":"Geometric Algebra Planes: Convex Implicit Neural Volumes","authors":"Irmak Sivgin, Sara Fridovich-Keil, Gordon Wetzstein, Mert Pilanci","journal":"arXiv preprint","year":"2024","doi":"","url":"https://arxiv.org/abs/2411.13525","keyPoints":"First convex optimization for implicit neural volumes; GA-Planes generalize existing representations; Low-rank plus low-resolution factorization","keyHypotheses":"INR training can be made convex through proper design; Convex optimization provides better guarantees than nonconvex","strengths":"Convex optimization guarantees; Generalizes many existing methods; Theoretical foundation","weaknesses":"May sacrifice some expressiveness; Complex mathematical framework","citation":"Sivgin, I., et al. (2024). Geometric algebra planes: Convex implicit neural volumes. arXiv preprint.","notes":"Novel theoretical advance making INR optimization convex - important for matrix reconstruction reliability","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"han2023nrff","title":"Multiscale Tensor Decomposition and Rendering Equation Encoding for View Synthesis","authors":"Kang Han, Wei Xiang","journal":"Conference proceedings","year":"2023","doi":"","url":"https://arxiv.org/abs/2303.03808","keyPoints":"Neural Radiance Feature Field with multiscale representation; Hierarchical spectral-spatial encoder; >1dB PSNR improvement","keyHypotheses":"Multiscale tensor decomposition improves reconstruction quality; Coarse-to-fine representation enables better feature learning","strengths":"Significant quality improvements; Multiscale approach; Faster convergence","weaknesses":"Added complexity; Hyperparameter sensitivity","citation":"Han, K., & Xiang, W. (2023). Multiscale tensor decomposition and rendering equation encoding for view synthesis. Conference proceedings.","notes":"Multiscale approaches relevant for hierarchical matrix reconstruction","addedDate":"2025-08-30T18:46:19.000Z"}
{"id":"cheng2025lowrank","title":"Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization","authors":"Zhengyun Cheng, Changhao Wang, Guanwen Zhang, Yi Xu, Wei Zhou, Xiangyang Ji","journal":"Submitted to IEEE TCSVT","year":"2025","doi":"","url":"https://arxiv.org/abs/2506.22134","keyPoints":"CP-based tensor decomposition for INRs; Schatten-p quasi-norm for sparsity; Jacobian regularization for smoothness","keyHypotheses":"CP decomposition provides interpretable tensor structure; Schatten-p norm improves sparse solutions; Jacobian regularization ensures smoothness","strengths":"Theoretical guarantees on excess risk; SVD-free regularization; Interpretable structure","weaknesses":"Complex optimization; Limited experimental validation","citation":"Cheng, Z., et al. (2025). Low-rank implicit neural representation via schatten-p quasi-norm and jacobian regularization. Submitted to IEEE TCSVT.","notes":"Direct application of low-rank tensor methods to INRs - highly relevant to matrix reconstruction","addedDate":"2025-08-30T18:46:19.000Z"}